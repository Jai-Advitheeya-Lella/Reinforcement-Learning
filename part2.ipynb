{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codecell 1: imports\n",
    "codecell 2 : defining deterministic environment\n",
    "codecell 3 : Running deterministic environment for 10 timesteps\n",
    "codecell 4: defining stochastic environment\n",
    "codecell 5 : running stochastic environment for 10 timestamps\n",
    "codecell 6: defining Q learning \n",
    "codecell 7 : test_policy(ignore)\n",
    "codecell 8:  Running Q Learnig for deterministic environment(it contains the plots replated to it)\n",
    "codecell 9: defining run_greedy_policy\n",
    "codecell 10: definining greedy policy and rendering \n",
    "code cell 11: runnin codecell 9 & 10 for deterministic environment\n",
    "codecell 12 : defining gamma_tuning\n",
    "codecell 13: running gamma tuning for deterministic env(contaains plots)\n",
    "\n",
    "codecell 14 : defining alpha_tuning\n",
    "codecell 15: running alpha tuning for deterministic env(contaains plots)\n",
    "Codecell 16: defining SARSA\n",
    "codecell 17: running sarsa for deterministic environment \n",
    "codecell 18: Running Q Learnig for stochastic  environment(it contains the plots related to it)\n",
    "codecell 19: runnin codecell 9 & 10 for stochastic environment\n",
    "codecell 20: running gamma tuning for Stochastic env(contaains plots)\n",
    "codecell 21: running alpha  tuning for Stochastic env(contaains plots)\n",
    "codecell 22: implementing SARSA for stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeCell 1\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeCell 2\n",
    "class DeterministicDroneDeliveryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 6\n",
    "        self.no_fly_zones = [(2,2), (2,3), (3,2), (3,3)]\n",
    "        self.action_space = gym.spaces.Discrete(5)  # Up, Down, Left, Right, Pickup/Dropoff\n",
    "        self.observation_space = gym.spaces.Tuple((\n",
    "            gym.spaces.Box(0, 5, (2,), dtype=int),  # Drone position\n",
    "            gym.spaces.Discrete(4),  # Carrying capacity (0-3 packages)\n",
    "            gym.spaces.MultiBinary(3)  # Delivery status for each customer\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.drone_pos = np.array([0, 0], dtype=int)\n",
    "        self.carrying = 3\n",
    "        self.num_customers = 3\n",
    "        self.delivery_locations = [(1, 2), (4, 4), (5, 0)]  # Fixed delivery locations\n",
    "        self.deliveries_completed = [False] * self.num_customers\n",
    "        self.steps = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        reward = -1  # Default step penalty\n",
    "        terminated = False\n",
    "\n",
    "        if action < 4:  # Movement\n",
    "            new_pos = self.drone_pos.copy()\n",
    "            if action == 0: new_pos[1] = min(5, new_pos[1]-1)  # Up\n",
    "            elif action == 1: new_pos[1] = max(0, new_pos[1]+1)  # Down\n",
    "            elif action == 2: new_pos[0] = max(0, new_pos[0]-1)  # Left\n",
    "            elif action == 3: new_pos[0] = min(5, new_pos[0]+1)  # Right\n",
    "\n",
    "            if self._is_valid(new_pos):\n",
    "                self.drone_pos = new_pos\n",
    "            else:\n",
    "                reward = -100  # No-fly zone penalty\n",
    "\n",
    "        elif action == 4:  # Pickup/Dropoff\n",
    "            if np.array_equal(self.drone_pos, [0, 0]):  # At warehouse\n",
    "                if self.carrying < 3:  # Pickup deliveries\n",
    "                    pass\n",
    "            else:\n",
    "                for i, loc in enumerate(self.delivery_locations):\n",
    "                    if np.array_equal(self.drone_pos, loc) and self.carrying > 0 and not self.deliveries_completed[i]:\n",
    "                        self.deliveries_completed[i] = True\n",
    "                        self.carrying -= 1\n",
    "                        reward = 100\n",
    "                        break\n",
    "\n",
    "        # Check if all tasks are completed\n",
    "        if all(self.deliveries_completed):\n",
    "            terminated = True\n",
    "            reward += 500  # Bonus for completing all tasks\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "    def _is_valid(self, pos):\n",
    "        return (tuple(pos) not in self.no_fly_zones \n",
    "                and 0 <= pos[0] < self.grid_size \n",
    "                and 0 <= pos[1] < self.grid_size)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.drone_pos.copy(), \n",
    "                self.carrying, \n",
    "                np.array(self.deliveries_completed))\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((6,6), '⬜')\n",
    "        for x,y in self.no_fly_zones:\n",
    "            grid[y,x] = '🟥'\n",
    "        \n",
    "        for i, (x, y) in enumerate(self.delivery_locations):\n",
    "            if not self.deliveries_completed[i]:\n",
    "                grid[y,x] = f'📦{i+1}'\n",
    "        \n",
    "        grid[self.drone_pos[1], self.drone_pos[0]] = '🚁' #+ str(self.carrying)\n",
    "        \n",
    "        print('\\n'.join([' '.join(row) for row in grid]))\n",
    "        print(f\"Carrying: {self.carrying} packages\")\n",
    "        print(f\"Deliveries completed: {sum(self.deliveries_completed)}/{self.num_customers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeCell 3\n",
    "# Example usage\n",
    "env = DeterministicDroneDeliveryEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(\"Initial State:\")\n",
    "env.render()\n",
    "\n",
    "for t in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    print(f\"\\nTimestep {t+1}\")\n",
    "    print(f\"State: {obs}\")\n",
    "    print(f\"Action: {['Up', 'Down', 'Left', 'Right', 'Pickup/Dropoff'][action]}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    env.render()\n",
    "    \n",
    "    obs = next_obs\n",
    "    if terminated:\n",
    "        print(\"Episode finished early.\")\n",
    "        break\n",
    "\n",
    "print(\"Random agent simulation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeCell 4\n",
    "class StochasticDroneDeliveryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 6\n",
    "        self.no_fly_zones = [(2,2), (2,3), (3,2), (3,3)]\n",
    "        self.action_space = gym.spaces.Discrete(5)  # Up, Down, Left, Right, Pickup/Dropoff\n",
    "        self.observation_space = gym.spaces.Tuple((\n",
    "            gym.spaces.Box(0, 5, (2,), dtype=int),  # Drone position\n",
    "            gym.spaces.Discrete(4),  # Carrying capacity (0-3 packages)\n",
    "            gym.spaces.MultiBinary(3)  # Delivery status for each customer\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.drone_pos = np.array([0, 0], dtype=int)\n",
    "        self.carrying = 3\n",
    "        self.num_customers = 3\n",
    "        self.delivery_locations = [(1, 2), (4, 4), (5, 0)]  # Fixed delivery locations\n",
    "        self.deliveries_completed = [False] * self.num_customers\n",
    "        self.steps = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        reward = -1  # Default step penalty\n",
    "        terminated = False\n",
    "\n",
    "        if action < 4:  # Movement\n",
    "            new_pos = self.drone_pos.copy()\n",
    "            if action == 0:  # Up\n",
    "                if np.random.rand() < 0.9:\n",
    "                    new_pos[1] = min(5, new_pos[1]-1)\n",
    "                else:\n",
    "                    new_pos = self._deviate(new_pos, action)\n",
    "            elif action == 1:  # Down\n",
    "                if np.random.rand() < 0.9:\n",
    "                    new_pos[1] = max(0, new_pos[1]+1)\n",
    "                else:\n",
    "                    new_pos = self._deviate(new_pos, action)\n",
    "            elif action == 2:  # Left\n",
    "                if np.random.rand() < 0.9:\n",
    "                    new_pos[0] = max(0, new_pos[0]-1)\n",
    "                else:\n",
    "                    new_pos = self._deviate(new_pos, action)\n",
    "            elif action == 3:  # Right\n",
    "                if np.random.rand() < 0.9:\n",
    "                    new_pos[0] = min(5, new_pos[0]+1)\n",
    "                else:\n",
    "                    new_pos = self._deviate(new_pos, action)\n",
    "\n",
    "            if self._is_valid(new_pos):\n",
    "                self.drone_pos = new_pos\n",
    "            else:\n",
    "                reward = -100  # No-fly zone penalty\n",
    "\n",
    "        elif action == 4:  # Pickup/Dropoff\n",
    "            if np.array_equal(self.drone_pos, [0, 0]):  # At warehouse\n",
    "                if self.carrying < 3:  # Pickup deliveries\n",
    "                    pass\n",
    "            else:\n",
    "                for i, loc in enumerate(self.delivery_locations):\n",
    "                    if np.array_equal(self.drone_pos, loc) and self.carrying > 0 and not self.deliveries_completed[i]:\n",
    "                        self.deliveries_completed[i] = True\n",
    "                        self.carrying -= 1\n",
    "                        reward = 100\n",
    "                        break\n",
    "\n",
    "        # Check if all tasks are completed\n",
    "        if all(self.deliveries_completed):\n",
    "            terminated = True\n",
    "            reward += 500  # Bonus for completing all tasks\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "    def _deviate(self, pos, action):\n",
    "        if action == 0 or action == 1:  # Up or Down\n",
    "            if np.random.rand() < 0.5:\n",
    "                pos[0] = max(0, pos[0]-1)  # Deviate Left\n",
    "            else:\n",
    "                pos[0] = min(5, pos[0]+1)  # Deviate Right\n",
    "        elif action == 2 or action == 3:  # Left or Right\n",
    "            if np.random.rand() < 0.5:\n",
    "                pos[1] = min(5, pos[1]+1)  # Deviate Down\n",
    "            else:\n",
    "                pos[1] = max(0, pos[1]-1)  # Deviate Up\n",
    "        return pos\n",
    "\n",
    "    def _is_valid(self, pos):\n",
    "        return (tuple(pos) not in self.no_fly_zones \n",
    "                and 0 <= pos[0] < self.grid_size \n",
    "                and 0 <= pos[1] < self.grid_size)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.drone_pos.copy(), \n",
    "                self.carrying, \n",
    "                np.array(self.deliveries_completed))\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((6,6), '⬜')\n",
    "        for x,y in self.no_fly_zones:\n",
    "            grid[y,x] = '🟥'\n",
    "        \n",
    "        for i, (x, y) in enumerate(self.delivery_locations):\n",
    "            if not self.deliveries_completed[i]:\n",
    "                grid[y,x] = f'📦{i+1}'\n",
    "        \n",
    "        grid[self.drone_pos[1], self.drone_pos[0]] = '🚁' #+ str(self.carrying)\n",
    "        \n",
    "        print('\\n'.join([' '.join(row) for row in grid]))\n",
    "        print(f\"Carrying: {self.carrying} packages\")\n",
    "        print(f\"Deliveries completed: {sum(self.deliveries_completed)}/{self.num_customers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeCell 5\n",
    "# Example usage\n",
    "env = StochasticDroneDeliveryEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(\"Initial State:\")\n",
    "env.render()\n",
    "\n",
    "for t in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    print(f\"\\nTimestep {t+1}\")\n",
    "    print(f\"State: {obs}\")\n",
    "    print(f\"Action: {['Up', 'Down', 'Left', 'Right', 'Pickup/Dropoff'][action]}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    env.render()\n",
    "    \n",
    "    obs = next_obs\n",
    "    if terminated:\n",
    "        print(\"Episode finished early.\")\n",
    "        break\n",
    "\n",
    "print(\"Random agent simulation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬜: Empty cell\n",
    "🟥: No-fly zone\n",
    "🚁: Drone (with carrying capacity)\n",
    "📦: Package delivery location\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeCell 6\n",
    "#Q Learning implementation\n",
    "# Initialize state space dimensions\n",
    "np.set_printoptions(threshold=np.inf,  # Show all elements\n",
    "                   linewidth=150,       # Wider display\n",
    "                   precision=2,         # 2 decimal places\n",
    "                   suppress=True)       # Don't use scientific notation\n",
    "grid_size = 6\n",
    "n_deliveries = 3\n",
    "n_states = grid_size * grid_size * (2**n_deliveries)  # 288 states \n",
    "n_actions = 5\n",
    "\n",
    "def state_to_index(state):\n",
    "    \n",
    "    pos, _, deliveries = state  # Ignore carrying value\n",
    "    pos_idx = pos[0] * grid_size + pos[1]  # Convert 2D position to 1D (0-35)\n",
    "    deliveries_idx = sum(int(d) << i for i, d in enumerate(deliveries))  # Binary encoding (0-7)\n",
    "    return int(pos_idx + (grid_size * grid_size * deliveries_idx))\n",
    "\n",
    "def q_learning(env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, \n",
    "              min_epsilon=0.01, num_episodes=10, showplts=False, return_rewards=False):\n",
    "    \n",
    "    # Initialize Q-table with reduced state space\n",
    "    q_table = np.zeros((n_states, n_actions), dtype=np.float32)\n",
    "    print(\"Initial Q-table:\")\n",
    "    print(q_table)\n",
    "    total_rewards = np.zeros(num_episodes, dtype=np.float32)\n",
    "    epsilon_values = np.zeros(num_episodes, dtype=np.float32)\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state_idx = state_to_index(state)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # Epsilon-greedy action selection\n",
    "                if np.random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = int(np.argmax(q_table[state_idx]))\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_state_idx = state_to_index(next_state)\n",
    "                \n",
    "                # Q-learning update (vectorized)\n",
    "                current_q = q_table[state_idx, action]\n",
    "                next_max_q = np.max(q_table[next_state_idx])\n",
    "                new_q = current_q + alpha * (reward + gamma * next_max_q - current_q)\n",
    "                q_table[state_idx, action] = new_q\n",
    "                #print(\"q_table updated\")\n",
    "                #print(reward)\n",
    "                state_idx = next_state_idx\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Update tracking variables\n",
    "            epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "            total_rewards[episode] = total_reward\n",
    "            epsilon_values[episode] = epsilon\n",
    "            \n",
    "            if (episode + 1) % 10 == 0:\n",
    "                avg_reward = np.mean(total_rewards[max(0, episode-9):episode+1])\n",
    "                print(f\"Episode {episode+1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "        if showplts:\n",
    "            plot_training_results(total_rewards, epsilon_values, num_episodes)\n",
    "        \n",
    "        print(\"Full Q-table:\")\n",
    "        print(q_table)\n",
    "        if return_rewards:\n",
    "            return q_table, total_rewards\n",
    "        return q_table\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def plot_training_results(total_rewards, epsilon_values, num_episodes):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(np.arange(num_episodes), total_rewards, 'b-')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.set_title('Rewards per Episode')\n",
    "    \n",
    "    # Plot smoothed rewards\n",
    "    window_size = min(10, num_episodes)\n",
    "    smoothed_rewards = np.convolve(total_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    ax1.plot(np.arange(len(smoothed_rewards)), smoothed_rewards, 'r-', alpha=0.5, label='Smoothed')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot epsilon decay\n",
    "    ax2.plot(np.arange(num_episodes), epsilon_values, 'g-')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Epsilon Value')\n",
    "    ax2.set_title('Epsilon Decay')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 7\n",
    "def test_policy(env, q_table, num_episodes=100):\n",
    "    total_rewards = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "\n",
    "        while not terminated:\n",
    "            action = np.argmax(q_table[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "\n",
    "    avg_reward = total_rewards / num_episodes\n",
    "    print(f\"Average reward: {avg_reward}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 8\n",
    "env=DeterministicDroneDeliveryEnv()\n",
    "q_table=q_learning(env,num_episodes=100,showplts=True)\n",
    "\n",
    "# Save the Q-table/Policy table as a pickle file of the trained model\n",
    "with open('deterministic_env_q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeCell 9\n",
    "\n",
    "def run_greedy_policy(env, q_table, num_episodes=10):\n",
    "   \n",
    "    total_rewards = np.zeros(num_episodes, dtype=np.float32)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state_idx = state_to_index(state)  # Convert state to index\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "\n",
    "        while not terminated:\n",
    "            # Select action greedily from Q-table\n",
    "            action = int(np.argmax(q_table[state_idx]))\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state_idx = state_to_index(next_state)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state_idx = next_state_idx\n",
    "\n",
    "        total_rewards[episode] = total_reward\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(num_episodes), total_rewards, 'bo-', label='Episode Reward')\n",
    "    \n",
    "    # Add moving average\n",
    "    window_size = min(5, num_episodes)\n",
    "    moving_avg = np.convolve(total_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(np.arange(len(moving_avg)), moving_avg, 'r-', label='Moving Average')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Greedy Policy Evaluation Results')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return total_rewards.tolist()  # Convert to list for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CodeCell 10\n",
    "def run_greedy_policy_and_render(env, q_table, max_steps=100):\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    state_idx = state_to_index(state)\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    \n",
    "    # Store trajectory for analysis\n",
    "    trajectory = []\n",
    "    action_names = ['Up', 'Down', 'Left', 'Right', 'Pickup/Dropoff']\n",
    "    \n",
    "    print(\"Initial State:\")\n",
    "    env.render()\n",
    "    \n",
    "    while not terminated and step < max_steps:\n",
    "        # Get greedy action from Q-table\n",
    "        action = int(np.argmax(q_table[state_idx]))\n",
    "        trajectory.append((state, action))\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state_idx = state_to_index(next_state)\n",
    "        \n",
    "        # Update tracking variables\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        state_idx = next_state_idx\n",
    "        step += 1\n",
    "        \n",
    "        # Render environment state\n",
    "        print(f\"\\nStep {step}\")\n",
    "        print(f\"Action: {action_names[action]}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        print(f\"Current Total Reward: {total_reward}\")\n",
    "        env.render()\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"\\nEpisode Summary:\")\n",
    "    print(f\"Total Steps: {step}\")\n",
    "    print(f\"Final Reward: {total_reward}\")\n",
    "    print(f\"Terminated: {terminated}\")\n",
    "    \n",
    "    if step >= max_steps:\n",
    "        print(\"Warning: Episode reached maximum steps!\")\n",
    "    \n",
    "    # Visualize trajectory\n",
    "    '''\n",
    "    if hasattr(env, 'grid_size'):\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.grid(True)\n",
    "        plt.title('Agent Trajectory')\n",
    "        \n",
    "        # Plot trajectory\n",
    "        traj_states = np.array([t[0][0] for t in trajectory])\n",
    "        plt.plot(traj_states[:, 1], traj_states[:, 0], 'b-o', alpha=0.6)\n",
    "        plt.plot(traj_states[0, 1], traj_states[0, 0], 'go', label='Start')\n",
    "        plt.plot(traj_states[-1, 1], traj_states[-1, 0], 'ro', label='End')\n",
    "        \n",
    "        plt.xlim(-0.5, env.grid_size - 0.5)\n",
    "        plt.ylim(-0.5, env.grid_size - 0.5)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    '''\n",
    "    \n",
    "    return {\n",
    "        'total_reward': total_reward,\n",
    "        'steps': step,\n",
    "        'trajectory': trajectory,\n",
    "        'terminated': terminated\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 11\n",
    "env=DeterministicDroneDeliveryEnv()\n",
    "# Run the greedy policy and plot the results\n",
    "run_greedy_policy(env, q_table, num_episodes=10)\n",
    "# Run the greedy policy and render the results\n",
    "run_greedy_policy_and_render(env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 12\n",
    "def gamma_tuning(env, num_episodes=100):\n",
    "    \n",
    "    # Select practical gamma values\n",
    "    gamma_values = [0.9, 0.95, 0.99]  # Common values for discounted rewards\n",
    "    results = {}\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        print(f\"\\nTraining with gamma = {gamma}\")\n",
    "        \n",
    "        # Train Q-learning with current gamma\n",
    "        q_table, training_rewards = q_learning(\n",
    "            env=env,\n",
    "            gamma=gamma,\n",
    "            alpha=0.1,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.995,\n",
    "            min_epsilon=0.01,\n",
    "            num_episodes=num_episodes,\n",
    "            showplts=True,\n",
    "            return_rewards=True\n",
    "        )\n",
    "        #print(f\"Final Q-table : {q_table}\")\n",
    "        # Run greedy policy\n",
    "        \n",
    "        \n",
    "        results[gamma] = {\n",
    "            'q_table': q_table,\n",
    "            'mean_reward': np.mean(training_rewards[-10:]),  # Last 10 episodes\n",
    "            'std_reward': np.std(training_rewards[-10:])\n",
    "        }\n",
    "    run_greedy_policy(env, q_table, num_episodes=10)\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot mean rewards with error bars\n",
    "    gammas = list(results.keys())\n",
    "    means = [results[g]['mean_reward'] for g in gammas]\n",
    "    stds = [results[g]['std_reward'] for g in gammas]\n",
    "    \n",
    "    plt.errorbar(gammas, means, yerr=stds, fmt='bo-', capsize=5, \n",
    "                label='Mean Reward ± Std')\n",
    "    \n",
    "    plt.xlabel('Gamma Value')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Q-Learning Performance vs Gamma')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults Summary:\")\n",
    "    for gamma in gamma_values:\n",
    "        print(f\"\\nGamma = {gamma}\")\n",
    "        print(f\"Mean Reward: {results[gamma]['mean_reward']:.2f}\")\n",
    "        print(f\"Std Reward: {results[gamma]['std_reward']:.2f}\")\n",
    "    \n",
    "    # Return best gamma\n",
    "    best_gamma = max(results.keys(), key=lambda g: results[g]['mean_reward'])\n",
    "    print(f\"\\nBest gamma value: {best_gamma}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = gamma_tuning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 13\n",
    "env= DeterministicDroneDeliveryEnv()\n",
    "results = gamma_tuning(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 14\n",
    "def epsilon_decay_tuning(env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate Q-learning performance with different epsilon decay values\n",
    "    \n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        num_episodes: Number of episodes for each epsilon decay evaluation\n",
    "    Returns:\n",
    "        dict: Results for each epsilon decay value\n",
    "    \"\"\"\n",
    "    # Select practical epsilon decay values\n",
    "    epsilon_decay_values = [0.99, 0.995, 0.999]  # Common values for epsilon decay\n",
    "    results = {}\n",
    "    \n",
    "    for epsilon_decay in epsilon_decay_values:\n",
    "        print(f\"\\nTraining with epsilon decay = {epsilon_decay}\")\n",
    "        \n",
    "        # Train Q-learning with current epsilon decay\n",
    "        q_table, training_rewards = q_learning(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            alpha=0.1,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=epsilon_decay,\n",
    "            min_epsilon=0.01,\n",
    "            num_episodes=num_episodes,\n",
    "            showplts=True,\n",
    "            return_rewards=True\n",
    "        )\n",
    "        \n",
    "        # Run greedy policy\n",
    "        run_greedy_policy(env, q_table, num_episodes=10)\n",
    "        \n",
    "        results[epsilon_decay] = {\n",
    "            'q_table': q_table,\n",
    "            'mean_reward': np.mean(training_rewards[-10:]),  # Last 10 episodes\n",
    "            'std_reward': np.std(training_rewards[-10:])\n",
    "        }\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot mean rewards with error bars\n",
    "    decays = list(results.keys())\n",
    "    means = [results[d]['mean_reward'] for d in decays]\n",
    "    stds = [results[d]['std_reward'] for d in decays]\n",
    "    \n",
    "    plt.errorbar(decays, means, yerr=stds, fmt='bo-', capsize=5, \n",
    "                label='Mean Reward ± Std')\n",
    "    \n",
    "    plt.xlabel('Epsilon Decay Value')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Q-Learning Performance vs Epsilon Decay')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults Summary:\")\n",
    "    for epsilon_decay in epsilon_decay_values:\n",
    "        print(f\"\\nEpsilon Decay = {epsilon_decay}\")\n",
    "        print(f\"Mean Reward: {results[epsilon_decay]['mean_reward']:.2f}\")\n",
    "        print(f\"Std Reward: {results[epsilon_decay]['std_reward']:.2f}\")\n",
    "    \n",
    "    # Return best epsilon decay\n",
    "    best_decay = max(results.keys(), key=lambda d: results[d]['mean_reward'])\n",
    "    print(f\"\\nBest epsilon decay value: {best_decay}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 15\n",
    "env= DeterministicDroneDeliveryEnv()\n",
    "results = epsilon_decay_tuning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 16\n",
    "def sarsa(env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, \n",
    "          min_epsilon=0.01, num_episodes=10, showplts=False, return_rewards=False):\n",
    "   \n",
    "    # Initialize Q-table\n",
    "    q_table = np.zeros((n_states, n_actions), dtype=np.float32)\n",
    "    print(\"Initial Q-table:\")\n",
    "    print(q_table)\n",
    "    total_rewards = np.zeros(num_episodes, dtype=np.float32)\n",
    "    epsilon_values = np.zeros(num_episodes, dtype=np.float32)\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state_idx = state_to_index(state)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # Select first action using epsilon-greedy\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = int(np.argmax(q_table[state_idx]))\n",
    "            \n",
    "            while not done:\n",
    "                # Take action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_state_idx = state_to_index(next_state)\n",
    "                \n",
    "                # Select next action using epsilon-greedy (SARSA is on-policy)\n",
    "                if np.random.random() < epsilon:\n",
    "                    next_action = env.action_space.sample()\n",
    "                else:\n",
    "                    next_action = int(np.argmax(q_table[next_state_idx]))\n",
    "                \n",
    "                # SARSA update\n",
    "                current_q = q_table[state_idx, action]\n",
    "                next_q = q_table[next_state_idx, next_action]\n",
    "                q_table[state_idx, action] = current_q + alpha * (\n",
    "                    reward + gamma * next_q - current_q\n",
    "                )\n",
    "                \n",
    "                # Update state and action\n",
    "                state_idx = next_state_idx\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Update tracking variables\n",
    "            epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "            total_rewards[episode] = total_reward\n",
    "            epsilon_values[episode] = epsilon\n",
    "            \n",
    "            if (episode + 1) % 10 == 0:\n",
    "                avg_reward = np.mean(total_rewards[max(0, episode-9):episode+1])\n",
    "                print(f\"Episode {episode+1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "        if showplts:\n",
    "            plot_training_results(total_rewards, epsilon_values, num_episodes)\n",
    "        print(\"Full Q-table:\")\n",
    "        print(q_table)\n",
    "        if return_rewards:\n",
    "            return q_table, total_rewards\n",
    "        return q_table\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codeCell 17\n",
    "env = DeterministicDroneDeliveryEnv()\n",
    "q_table = sarsa(env, num_episodes=100, showplts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 18\n",
    "env=StochasticDroneDeliveryEnv()\n",
    "q_table=q_learning(env,num_episodes=100,showplts=True)\n",
    "\n",
    "# Save the Q-table/Policy table as a pickle file of the trained model\n",
    "with open('stochastic_env_q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 19\n",
    "env=StochasticDroneDeliveryEnv()\n",
    "# Run the greedy policy and plot the results\n",
    "run_greedy_policy(env, q_table, num_episodes=10)\n",
    "# Run the greedy policy and render the results\n",
    "run_greedy_policy_and_render(env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 20\n",
    "env= StochasticDroneDeliveryEnv()\n",
    "results = gamma_tuning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 21\n",
    "env= StochasticDroneDeliveryEnv()\n",
    "results = epsilon_decay_tuning(env)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeCell 22\n",
    "env = StochasticDroneDeliveryEnv()\n",
    "q_table = sarsa(env, num_episodes=100, showplts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
